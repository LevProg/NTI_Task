{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MaryBert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LevProg/NTI_Task/blob/main/MaryBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siD3HTVDTEaW"
      },
      "source": [
        "Проверяем, что включен GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0owd5wYFS8kC",
        "outputId": "f68c5495-f869-4445-b4e9-15e594c8a766"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "# If there's a GPU available...\r\n",
        "if torch.cuda.is_available():    \r\n",
        "\r\n",
        "    # Tell PyTorch to use the GPU.    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "\r\n",
        "# If not...\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUnXjCtqTSpS",
        "outputId": "3651fb85-dd89-4202-ba3c-0374f07c01f5"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IkGqmpqTu-A"
      },
      "source": [
        "Скачиваем данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Mg3mHxYTXpy",
        "outputId": "16c17f9b-6352-4c33-9250-008089413a3e"
      },
      "source": [
        "!wget https://russiansuperglue.com/tasks/download/MuSeRC\r\n",
        "!mv MuSeRC MuSeRC.zip\r\n",
        "!unzip MuSeRC.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-10 20:07:58--  https://russiansuperglue.com/tasks/download/MuSeRC\n",
            "Resolving russiansuperglue.com (russiansuperglue.com)... 80.66.93.142\n",
            "Connecting to russiansuperglue.com (russiansuperglue.com)|80.66.93.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1196720 (1.1M) [application/zip]\n",
            "MuSeRC: Is a directory\n",
            "\n",
            "Cannot write to ‘MuSeRC’ (Success).\n",
            "mv: cannot overwrite non-directory 'MuSeRC.zip' with directory 'MuSeRC'\n",
            "Archive:  MuSeRC.zip\n",
            "replace MuSeRC/train.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: MuSeRC/train.jsonl      \n",
            "replace __MACOSX/MuSeRC/._train.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/MuSeRC/._train.jsonl  \n",
            "replace MuSeRC/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: MuSeRC/.DS_Store        \n",
            "replace __MACOSX/MuSeRC/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/MuSeRC/._.DS_Store  \n",
            "replace MuSeRC/test.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: MuSeRC/test.jsonl       \n",
            "replace __MACOSX/MuSeRC/._test.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/MuSeRC/._test.jsonl  \n",
            "replace MuSeRC/val.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: MuSeRC/val.jsonl        \n",
            "replace __MACOSX/MuSeRC/._val.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/MuSeRC/._val.jsonl  \n",
            "replace __MACOSX/._MuSeRC? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/._MuSeRC       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIHJx9zGXbsU"
      },
      "source": [
        "Загружаем данные, предобрабатываем, добавляем токены CLS и SEP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb3V8cv0T6HY"
      },
      "source": [
        "import json\r\n",
        "import re\r\n",
        "\r\n",
        "FEATURES_COUNT = 230\r\n",
        "\r\n",
        "def clean_text(text):\r\n",
        "\r\n",
        "    text = text.lower()\r\n",
        "    \r\n",
        "    pattern = '\\(\\d+\\)'\r\n",
        "    text = re.sub(pattern, '', text)\r\n",
        "\r\n",
        "    text = text.replace('\\xad', '')\r\n",
        "    text = text.replace('\\u00ad', '')\r\n",
        "    text = text.replace('\\N{SOFT HYPHEN}', '')\r\n",
        "    \r\n",
        "    text = text.replace('«', '')\r\n",
        "    text = text.replace('»', '')  \r\n",
        "    text = text.replace('...', '') \r\n",
        "    text = text.replace('—', '') \r\n",
        "\r\n",
        "    text = re.sub(r'[^\\w\\s]','',text)\r\n",
        "\r\n",
        "    return text\r\n",
        "\r\n",
        "def text_splitter(text, amount=FEATURES_COUNT): \r\n",
        "    tokens = clean_text(text).split(' ')\r\n",
        "    new_text = ' '.join(tokens[-amount:])\r\n",
        "    return new_text\r\n",
        "\r\n",
        "def get_X_y_for_bert(data_json_file):\r\n",
        "    X, y = [], []\r\n",
        "    with open(data_json_file, 'r', encoding='utf8') as json_file:\r\n",
        "        json_list = list(json_file)\r\n",
        "        #print(json_list[0])\r\n",
        "        for json_str in json_list:\r\n",
        "            item = json.loads(json_str)\r\n",
        "            text = item['passage']['text']\r\n",
        "            questions = item['passage']['questions']\r\n",
        "            for q in questions:\r\n",
        "                query = q['question']\r\n",
        "                ans = q['answers']\r\n",
        "                for a in ans:\r\n",
        "                    #X.append((text_splitter(text+' textquestionseparator '+query+' '+a['text'])).replace('textquestionseparator', '[SEP]'))\r\n",
        "                    X.append('[CLS] '+ (text_splitter(query+' '+a['text']+' textquestionseparator '+text)).replace('textquestionseparator', '[SEP]'))\r\n",
        "                    y.append(a['label'])\r\n",
        "    return X, y\r\n",
        "\r\n",
        "def get_X(data_json_file):\r\n",
        "    X = []\r\n",
        "    with open(data_json_file, 'r', encoding='utf8') as json_file:\r\n",
        "        json_list = list(json_file)\r\n",
        "        #print(json_list[0])\r\n",
        "        for json_str in json_list:\r\n",
        "            item = json.loads(json_str)\r\n",
        "            text = item['passage']['text']\r\n",
        "            questions = item['passage']['questions']\r\n",
        "            for q in questions:\r\n",
        "                query = q['question']\r\n",
        "                ans = q['answers']\r\n",
        "                for a in ans:\r\n",
        "                    X.append('[CLS] '+ (text_splitter(query+' '+a['text']+' textquestionseparator '+text)).replace('textquestionseparator', '[SEP]'))\r\n",
        "                    \r\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hufwyrNeUIjo"
      },
      "source": [
        "X_train, y_train = get_X_y_for_bert('MuSeRC/train.jsonl')\r\n",
        "X_valid, y_valid = get_X_y_for_bert('MuSeRC/val.jsonl')\r\n",
        "X_test = get_X('MuSeRC/test.jsonl') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "-u-HWnojVbCK",
        "outputId": "84f5cdb8-eda7-4e3b-fe90-bf833a487542"
      },
      "source": [
        "X_train[100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] как николай хабибулин принес своей команде победу восьмого ноября в чикаго он поймал все шайбы противника [SEP]  российский вратарь клуба нхл эдмонтон ойлерс николай хабибулин принес своей команде победу в матче с действующим обладателем кубка стэнли чикаго блэк хокс  эта игра состоялась в ночь на 8 ноября в чикаго и завершилась со счетом 21 в пользу эдмонтона  хабибулин отразил 26 из 27 бросков сообщает официальный сайт нхл  единственную шайбу в ворота чикаго в середине первого периода забросил фернандо писани  эдмонтон забил два гола на восьмой минуте третьего периода с интервалом в 14 секунд  отличились крис кертис и сэм ганье  после этой победы у ойлерс стало десять очков в 12 матчах и команда осталась на последнем месте в западной конференции  хабибулин в нынешнем сезоне принял участие в 11 матчах  в среднем он отражает 905 процента бросков по своим воротам и пропускает по 31 шайбы за игру  один матч россиянин отстоял на ноль  из других матчей сыгранных в ночь на 8 ноября стоит отметить победу вашингтон кэпиталс на филадельфией флайерс 32 в овертайме  нападающий вашингтона александр семин забросил шайбу а его одноклубник александр овечкин отдал голевой пас  один из голов филадельфии на счету николая жердева вратарь флайерс сергей бобровский отразил 36 из 39 бросков'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oSebvXTXZfd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amnuC1TBY7ai"
      },
      "source": [
        "Меняем предобученный англоязычный берт из примера на мультиязычный (можно попробовать еще русский)     \r\n",
        "В берте токен это не только слово, но может и быть часть слова (см. выводы токенайзера далее)   \r\n",
        "Если не поменяем модель, то токеном будет каждая буква"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C3kIZpOYDKi",
        "outputId": "50f4d7fd-1210-4465-901c-3e645ec79181"
      },
      "source": [
        "from transformers import BertTokenizer\r\n",
        "\r\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\r\n",
        "\r\n",
        "# Load the BERT tokenizer.\r\n",
        "print('Loading BERT tokenizer...')\r\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\r\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEBZ3k0XYCVw",
        "outputId": "bfcee1cb-d4cb-4e5a-b3d0-eb2cfb5fb07f"
      },
      "source": [
        "# Print the original sentence.\r\n",
        "print(' Original: ', X_train[100])\r\n",
        "\r\n",
        "# Print the sentence split into tokens.\r\n",
        "print('Tokenized: ', tokenizer.tokenize(X_train[100]))\r\n",
        "\r\n",
        "# Print the sentence mapped to token ids.\r\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(X_train[100])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  [CLS] как николай хабибулин принес своей команде победу восьмого ноября в чикаго он поймал все шайбы противника [SEP]  российский вратарь клуба нхл эдмонтон ойлерс николай хабибулин принес своей команде победу в матче с действующим обладателем кубка стэнли чикаго блэк хокс  эта игра состоялась в ночь на 8 ноября в чикаго и завершилась со счетом 21 в пользу эдмонтона  хабибулин отразил 26 из 27 бросков сообщает официальный сайт нхл  единственную шайбу в ворота чикаго в середине первого периода забросил фернандо писани  эдмонтон забил два гола на восьмой минуте третьего периода с интервалом в 14 секунд  отличились крис кертис и сэм ганье  после этой победы у ойлерс стало десять очков в 12 матчах и команда осталась на последнем месте в западной конференции  хабибулин в нынешнем сезоне принял участие в 11 матчах  в среднем он отражает 905 процента бросков по своим воротам и пропускает по 31 шайбы за игру  один матч россиянин отстоял на ноль  из других матчей сыгранных в ночь на 8 ноября стоит отметить победу вашингтон кэпиталс на филадельфией флайерс 32 в овертайме  нападающий вашингтона александр семин забросил шайбу а его одноклубник александр овечкин отдал голевой пас  один из голов филадельфии на счету николая жердева вратарь флайерс сергей бобровский отразил 36 из 39 бросков\n",
            "Tokenized:  ['[CLS]', 'как', 'ни', '##кол', '##аи', 'х', '##аби', '##бул', '##ин', 'при', '##нес', 'св', '##ое', '##и', 'команде', 'победу', 'во', '##сь', '##мого', 'ноября', 'в', 'чи', '##ка', '##го', 'он', 'по', '##има', '##л', 'все', 'ша', '##иб', '##ы', 'противника', '[SEP]', 'рос', '##сии', '##ски', '##и', 'в', '##рата', '##рь', 'клуба', 'н', '##х', '##л', 'э', '##д', '##монт', '##он', 'о', '##иле', '##рс', 'ни', '##кол', '##аи', 'х', '##аби', '##бул', '##ин', 'при', '##нес', 'св', '##ое', '##и', 'команде', 'победу', 'в', 'матче', 'с', 'де', '##ист', '##вую', '##щим', 'обл', '##ада', '##телем', 'кубка', 'ст', '##эн', '##ли', 'чи', '##ка', '##го', 'б', '##л', '##эк', 'х', '##ок', '##с', 'эта', 'игра', 'состоялась', 'в', 'ночь', 'на', '8', 'ноября', 'в', 'чи', '##ка', '##го', 'и', 'за', '##вер', '##шила', '##сь', 'со', 'с', '##чет', '##ом', '21', 'в', 'пользу', 'э', '##д', '##монт', '##она', 'х', '##аби', '##бул', '##ин', 'от', '##разил', '26', 'из', '27', 'бр', '##ос', '##ков', 'со', '##об', '##щает', 'о', '##фи', '##циал', '##ьны', '##и', 'са', '##ит', 'н', '##х', '##л', 'един', '##ственную', 'ша', '##иб', '##у', 'в', 'ворота', 'чи', '##ка', '##го', 'в', 'середине', 'первого', 'периода', 'за', '##бр', '##ос', '##ил', 'ф', '##ерна', '##ндо', 'п', '##иса', '##ни', 'э', '##д', '##монт', '##он', 'забил', 'два', 'гола', 'на', 'во', '##сь', '##мо', '##и', 'минут', '##е', 'третьего', 'периода', 'с', 'ин', '##тер', '##вало', '##м', 'в', '14', 'секунд', 'от', '##ли', '##чили', '##сь', 'к', '##рис', 'к', '##ерт', '##ис', 'и', 'с', '##эм', 'га', '##нь', '##е', 'после', 'это', '##и', 'победы', 'у', 'о', '##иле', '##рс', 'стало', 'десять', 'очков', 'в', '12', 'матчах', 'и', 'команда', 'ос', '##тала', '##сь', 'на', 'после', '##днем', 'месте', 'в', 'западно', '##и', 'конференции', 'х', '##аби', '##бул', '##ин', 'в', 'ныне', '##шне', '##м', 'сезоне', 'принял', 'участие', 'в', '11', 'матчах', 'в', 'среднем', 'он', 'от', '##ра', '##жает', '905', 'про', '##цент', '##а', 'бр', '##ос', '##ков', 'по', 'своим', 'ворота', '##м', 'и', 'про', '##пуска', '##ет', 'по', '31', 'ша', '##иб', '##ы', 'за', 'игру', 'один', 'матч', 'рос', '##сия', '##нин', 'отс', '##то', '##ял', 'на', 'но', '##ль', 'из', 'других', 'матче', '##и', 'с', '##ыг', '##ран', '##ных', 'в', 'ночь', 'на', '8', 'ноября', 'стоит', 'от', '##метить', 'победу', 'ва', '##шин', '##гт', '##он', 'к', '##э', '##пит', '##ал', '##с', 'на', 'ф', '##ила', '##дель', '##фи', '##еи', 'ф', '##ла', '##ие', '##рс', '32', 'в', 'ове', '##рта', '##име', 'напад', '##аю', '##щи', '##и', 'ва', '##шин', '##гт', '##она', 'але', '##кса', '##нд', '##р', 'семи', '##н', 'за', '##бр', '##ос', '##ил', 'ша', '##иб', '##у', 'а', 'его', 'одно', '##кл', '##уб', '##ник', 'але', '##кса', '##нд', '##р', 'ове', '##чки', '##н', 'от', '##дал', 'гол', '##ево', '##и', 'па', '##с', 'один', 'из', 'голов', 'ф', '##ила', '##дель', '##фии', 'на', 'с', '##чет', '##у', 'ни', '##кол', '##ая', 'жерде', '##ва', 'в', '##рата', '##рь', 'ф', '##ла', '##ие', '##рс', 'се', '##рг', '##еи', 'бо', '##бр', '##овски', '##и', 'от', '##разил', '36', 'из', '39', 'бр', '##ос', '##ков']\n",
            "Token IDs:  [101, 10949, 19544, 48613, 35912, 562, 86840, 104037, 12029, 10913, 38157, 37629, 17117, 10191, 48769, 42320, 10439, 11833, 64608, 15452, 543, 17891, 10521, 10990, 11060, 10297, 17804, 10517, 13686, 79703, 98446, 10292, 28471, 102, 28171, 28445, 11434, 10191, 543, 38790, 28301, 22871, 554, 10353, 10517, 570, 10746, 67196, 11579, 555, 50751, 45760, 19544, 48613, 35912, 562, 86840, 104037, 12029, 10913, 38157, 37629, 17117, 10191, 48769, 42320, 543, 30158, 558, 11323, 24142, 72536, 23527, 58742, 36069, 25620, 81198, 15888, 16893, 10783, 17891, 10521, 10990, 542, 10517, 70426, 562, 11899, 10513, 43178, 28940, 49007, 543, 60092, 10122, 129, 15452, 543, 17891, 10521, 10990, 549, 10234, 32418, 60603, 11833, 10956, 558, 54697, 10364, 10296, 543, 79463, 570, 10746, 67196, 14614, 562, 86840, 104037, 12029, 10332, 90402, 10314, 10387, 10365, 109300, 17969, 13036, 10956, 33276, 41601, 555, 26245, 72459, 84435, 10191, 10868, 15811, 554, 10353, 10517, 23727, 72206, 79703, 98446, 10227, 543, 62516, 17891, 10521, 10990, 543, 39258, 26924, 26773, 10234, 85584, 17969, 13460, 561, 65674, 54274, 556, 51116, 10656, 570, 10746, 67196, 11579, 44225, 12500, 53117, 10122, 10439, 11833, 19900, 10191, 46920, 10205, 86691, 26773, 558, 27796, 15469, 38209, 10241, 543, 10247, 88312, 10332, 10783, 49964, 11833, 551, 72655, 551, 92190, 19120, 549, 558, 38659, 16616, 15266, 10205, 11921, 12999, 10191, 63726, 560, 555, 50751, 45760, 22048, 78999, 65764, 543, 10186, 34843, 549, 24052, 85854, 61782, 11833, 10122, 11921, 107631, 27889, 543, 100182, 10191, 51406, 562, 86840, 104037, 12029, 543, 26541, 92954, 10241, 22863, 30543, 16635, 543, 10193, 34843, 543, 67606, 11060, 10332, 11079, 47546, 70781, 12709, 60515, 10179, 109300, 17969, 13036, 10297, 28365, 62516, 10241, 549, 12709, 54288, 11613, 10297, 10413, 79703, 98446, 10292, 10234, 56762, 13713, 33054, 28171, 25269, 19676, 103229, 10752, 29065, 10122, 11279, 12118, 10387, 15597, 30158, 10191, 558, 32307, 16346, 10970, 543, 60092, 10122, 129, 15452, 75363, 10332, 88794, 42320, 12450, 28483, 37876, 11579, 551, 18510, 80465, 12743, 10513, 10122, 561, 16610, 70835, 26245, 36694, 561, 10674, 12686, 45760, 10842, 543, 46809, 26042, 54240, 78417, 103270, 19682, 10191, 12450, 28483, 37876, 14614, 13166, 57191, 16821, 10519, 76463, 10267, 10234, 85584, 17969, 13460, 79703, 98446, 10227, 541, 10933, 41543, 53869, 40124, 11718, 13166, 57191, 16821, 10519, 46809, 14995, 10267, 10332, 28354, 25556, 42500, 10191, 12634, 10513, 13713, 10387, 64371, 561, 16610, 70835, 100380, 10122, 558, 54697, 10227, 19544, 48613, 14655, 46070, 10852, 543, 38790, 28301, 561, 10674, 12686, 45760, 10277, 54841, 36694, 18481, 85584, 91945, 10191, 10332, 90402, 11055, 10387, 11303, 109300, 17969, 13036]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvgyYUn5aTER"
      },
      "source": [
        "Максимальная длина одного текста - 512 токенов. Проверяем, что все в порядке:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqxJ5nVvXZOa",
        "outputId": "afeec222-1cb9-4525-a863-1018153fed46"
      },
      "source": [
        "max_len = 0\r\n",
        "\r\n",
        "# For every sentence...\r\n",
        "for sent in X_train:\r\n",
        "\r\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\r\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\r\n",
        "\r\n",
        "    # Update the maximum sentence length.\r\n",
        "    max_len = max(max_len, len(input_ids))\r\n",
        "\r\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orHvpfy_ayGg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQM6RrNfax7c"
      },
      "source": [
        "def tokenize_texts(sentences):\r\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\r\n",
        "  input_ids = []\r\n",
        "  attention_masks = []\r\n",
        "\r\n",
        "  # For every sentence...\r\n",
        "  for sent in sentences:\r\n",
        "      # `encode_plus` will:\r\n",
        "      #   (1) Tokenize the sentence.\r\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\r\n",
        "      #   (3) Append the `[SEP]` token to the end.\r\n",
        "      #   (4) Map tokens to their IDs.\r\n",
        "      #   (5) Pad or truncate the sentence to `max_length`\r\n",
        "      #   (6) Create attention masks for [PAD] tokens.\r\n",
        "      encoded_dict = tokenizer.encode_plus(\r\n",
        "                          sent,                      # Sentence to encode.\r\n",
        "                          add_special_tokens = False, # Add '[CLS]' and '[SEP]'\r\n",
        "                          max_length = 512,           # Pad & truncate all sentences.\r\n",
        "                          pad_to_max_length = True,\r\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\r\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\r\n",
        "                    )\r\n",
        "      \r\n",
        "      # Add the encoded sentence to the list.    \r\n",
        "      input_ids.append(encoded_dict['input_ids'])\r\n",
        "      \r\n",
        "      # And its attention mask (simply differentiates padding from non-padding).\r\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\r\n",
        "\r\n",
        "  # Convert the lists into tensors.\r\n",
        "  input_ids = torch.cat(input_ids, dim=0)\r\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\r\n",
        "\r\n",
        "  # Print sentence 0, now as a list of IDs.\r\n",
        "  print('Original: ', sentences[0])\r\n",
        "  print('Token IDs:', input_ids[0])\r\n",
        "\r\n",
        "  return input_ids, attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlFj4r6MbDGx",
        "outputId": "c3d7de0a-5eea-4f78-8bac-d3e098ea8a14"
      },
      "source": [
        "X_train, X_train_masks = tokenize_texts(X_train)\r\n",
        "X_valid, X_valid_masks = tokenize_texts(X_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  [CLS] где бегала шпана в парке [SEP]  но люди не могут существовать без природы поэтому в парке стояли железобетонные скамейки  деревянные моментально ломали  в парке бегали ребятишки водилась шпана которая развлекалась игрой в карты пьянкой драками иногда насмерть  имали они тут и девок  верховодил шпаной артемкамыло с вспененной белой головой  людочка сколько ни пыталась усмирить лохмотья на буйной голове артемки ничего у неё не получалось  его кудри издали напоминавшие мыльную пену изблизя оказались что липкие рожки из вокзальной столовой  сварили их бросили комком в пустую тарелку так они слипшиеся неподъёмно и лежали  да и не ради причёски приходил парень к людочке  как только её руки становились занятыми ножницами и расчёской артемка начинал хватать её за разные места  людочка сначала увёртывалась от хватких рук артемки а когда не помогло стукнула его машинкой по голове и пробила до крови пришлось лить йод на голову ухажористого человека  артемка заулюлюкал и со свистом стал ловить воздух  с тех пор домогания свои хулиганские прекратил более того шпане повелел людочку не трогать\n",
            "Token IDs: tensor([   101,  12252,    542,  48372,  10674,  13568,  14622,    543,  31667,\n",
            "         10205,    102,  11279,  28469,  10375,  22293,  10587,  57820,  27013,\n",
            "         13012,  70678,  33910,    543,  31667,  10205, 108804,  33773,  11815,\n",
            "         11851,  58607,  67218,  11579,  11194,    558,  31071,  36694,  10648,\n",
            "         56572,  18971,  11194,  51718,  19619,  30977,  82918,    543,  31667,\n",
            "         10205,    542,  48372,  10783,    557,  59373,  30010,  20346,  24933,\n",
            "         20292,  13568,  14622,  16510,  17257,  10541,  30150,  69445,  31064,\n",
            "         26891,    543,  73985,    556,  15609,  38072,  10191,  16522,  34411,\n",
            "         10508,  38223,  32001,  33930,  11258,  82450,  14274,  26514,    549,\n",
            "        109910,  11899,    543,  57935,  20007,  72391,  13568,  20822,  10191,\n",
            "         40585,  10696,  63861,  15657,  11602,    558,    543,  10513,  34210,\n",
            "         56717,  10191,  32609,  26891,  64371,  26891,    552,  10593,  17961,\n",
            "         15116,    558, 101351,  19544,    556,  86360,  20292,    560,  10513,\n",
            "         41679,  15356,  30977,  10353, 104722,  15609,  10122,  18261,  31587,\n",
            "         10191,  64371,  10205,  40585,  10696,  54929,  66581,    560,  10375,\n",
            "         10205,  10375,  91258,  52464,  11833,  10933,    551,  23147,  11777,\n",
            "         21529,  15710,  10122,  91363,  13139,  55580,  35818,  39610,  42607,\n",
            "         10227,  10387,  61394,  39457,  10385,  96061,  10791,  33190,  46953,\n",
            "         10205,    557,  10316,  34436,  10387,  10439,  10510,  42276,  24675,\n",
            "         10191, 108804,  18359,  26891,  37629,  30090,  10783,  12064, 109300,\n",
            "         17969,  16878,  59781,  10241,  13393,    543,    556,  19954,  56214,\n",
            "         10475,  67244,  11191,  12123,  14274,  52399,  30637,  78152,  10375,\n",
            "         53204,  10746,  30318,  12579,  10636,    549,  94693,  48873,  10405,\n",
            "           549,  10375,  18701,  10913,  24373,  10913,  82005,  66457,  19557,\n",
            "           551,    552,  10593,  17961,  19347,  10949,  13721,  75431,  50365,\n",
            "         66345,  71728,  11833,  10234,  87455,  21473,  11279,  40703,  34237,\n",
            "           549,    557,  18291,  53562,  10191,  40585,  10696,  63861,  27353,\n",
            "         12743,    562,  35282,  11258,  75431,  10234,  79162,  19442,    552,\n",
            "         10593,  17961,  15116,  51174,    560,  32418,  12202,  63967,  10332,\n",
            "           562,  18741,  18050,  84043,  40585,  10696,  54929,    541,  15283,\n",
            "         10375,  96358,  18442,  11602,  15888,  20216, 103614,  10933,  51290,\n",
            "         11623,  10191,  10297,  64371,  10205,    549,  12709,  50519,  10344,\n",
            "         83191,  64336,  99197,  10851,    549,  16625,  10122,  83603,    560,\n",
            "         16183,  52834,  62261,  12470,  18035,  40585,  10696,  63861,  10234,\n",
            "         18126,  10593,  21190,  48225,    549,  10956,  60763,  54853,  13462,\n",
            "         30977,  60797,  10439,  86537,    558,  32323,  41436,  21487,  36666,\n",
            "         12268,  22003,    562,  30102,  14814,  14354,  38494,  10510,  77002,\n",
            "         13106,  12409,  13568,  30201,  30148,  31851,  10517,    552,  10593,\n",
            "         17961,  33755,  10375,    559,  84349,  11258,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0])\n",
            "Original:  [CLS] почему солженицына перевозили спецконвоем так перевозили особо важных заключенных [SEP]  самый первый остров архипелага возник в 1923 году на месте соловецкого монастыря  затем появились тоны  тюрьмы особого назначения и этапы  люди попадали на архипелаг разными способами в вагонзаках на баржах пароходах и пешими этапами  в тюрьмы арестованных доставляли в воронках  фургончиках чёрного цвета  роль портов архипелага играли пересылки временные лагеря состоящие из палаток землянок бараков или участков земли под открытым небом  на всех пересылках держать политических в узде помогали специально отобранные урки или социально близкие  солженицын побывал на пересылке красная пресня в 1945 году  эмигранты крестьяне и малые народы перевозили красными эшелонами  чаще всего такие эшелоны останавливались на пустом месте посреди степи или тайги и осуждённые сами строили лагерь  особо важные заключённые в основном учёные перевозились спецконвоем  так перевозили и солженицына  он назвался ядерным физиком и после красной пресни его перевезли в бутырки\n",
            "Token IDs: tensor([   101,  10297,  96506,  26367,  84935,  12993,  10409,  61381,  42500,\n",
            "         14894,  10783,    558,  19820,  82648,  11579,  57798,  10241,  12123,\n",
            "         61381,  42500,  14894,  10783,  85854,  33276,  10316,  12450,  28010,\n",
            "         10234, 101032,  10970,    102,  19654,  10292,  10191,  61381,  18221,\n",
            "         10191,  34392,  40585,  22710,  91447,  11347,  10439,  68070,  10510,\n",
            "           543,  11521,  10495,  10122,  27889,  98691,  14149,  43938,  55482,\n",
            "         16907,  62468,  11663,  11307,    559,  10593,  28301,  15657,  85854,\n",
            "         33276,  12470,  76919,    549,  85081,  10292,  28469,  48138,  36069,\n",
            "         10783,  10122,  40585,  22710,  91447,  10823,  17257,  14350,    558,\n",
            "         97205,  12040,    543,  12450,  39170,  85924,  12588,  10122,  14748,\n",
            "         17254,  10353,  66457,  51823,  24369,  10353,    549,    556,  95135,\n",
            "         10508,  85081,  12040,    543,    559,  10593,  28301,  15657,  79589,\n",
            "         10970,  64634,  10541,  48499,    543,  10439,  23395,  20265,    561,\n",
            "         85902,  11579,  50329,  10353,  14816,  45168,  42128,  16611,  27999,\n",
            "         10433,  40585,  22710,  91447,  11347,  38448,  10191,  61381,  17488,\n",
            "         37721,  10648,  13638,  23997,  26670,  28826,  10385,  91908,  27684,\n",
            "         25778,  10387,  12634,  30828,  11899,  85710,  49062,  58425,  13036,\n",
            "         10880,    560,  45046,  93653,  32369,  11429,  60427,  13480,  39483,\n",
            "         10241,  10122,  16989,  61381,  17488,  37721,  20265,  90139,  59919,\n",
            "         71264,    543,  41733,  12265,  96358,  36666,  10783,  92909,  10332,\n",
            "         33276,  16346,  11194,    560,  46559,  10880,  97106,  76606,  27441,\n",
            "         26367,  84935,  12993,  10267,  10297,  18766,  18979,  10122,  61381,\n",
            "         17488,  37721,  11557,    551,  56680,  11548,  38494,  10513,  12751,\n",
            "           543,  10670,  10495,  83113,  69330,  41806,  89048,  10205,    549,\n",
            "         50818,  14578,  41461,  10292,  61381,  42500,  14894,  10783,    551,\n",
            "         56680,  14350,    570,  42171,  14614,  10508,  71447,  18275,  33673,\n",
            "           570,  42171,  34501,  85854,  36949,  10541,  35777,  17036,  10122,\n",
            "           556,  19954,  18925,  27889,  10297,  10513,  63796,  15888,  29749,\n",
            "         10191,  10880,  10475, 104695,    549,  85854,  10227,  12025,  16770,\n",
            "         11194,  77489,  19240,  26891,  10783,  26670,  28826,  10851,  85854,\n",
            "         33276,  10316,  12450,  47218,  10234, 101032,  11194,    543,  27434,\n",
            "           560,  54410,  10205,  61381,  42500,  14894,  17036,    558,  19820,\n",
            "         82648,  11579,  57798,  10241,  12123,  61381,  42500,  14894,  10783,\n",
            "           549,  26367,  84935,  12993,  10409,  11060,  80927,  10625,    572,\n",
            "         22605,  11692, 102733,  10241,    549,  11921,    551,  56680,  10636,\n",
            "         10191,  38494,  40543,  10933,  61381,  82713,  91408,    543,  18261,\n",
            "         55130,  10648,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4Q_AZDzc3_8",
        "outputId": "245cbf04-8cdb-4368-9d27-6ea7838e7b63"
      },
      "source": [
        "X_train[100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   101,  10949,  19544,  48613,  35912,    562,  86840, 104037,  12029,\n",
              "         10913,  38157,  37629,  17117,  10191,  48769,  42320,  10439,  11833,\n",
              "         64608,  15452,    543,  17891,  10521,  10990,  11060,  10297,  17804,\n",
              "         10517,  13686,  79703,  98446,  10292,  28471,    102,  28171,  28445,\n",
              "         11434,  10191,    543,  38790,  28301,  22871,    554,  10353,  10517,\n",
              "           570,  10746,  67196,  11579,    555,  50751,  45760,  19544,  48613,\n",
              "         35912,    562,  86840, 104037,  12029,  10913,  38157,  37629,  17117,\n",
              "         10191,  48769,  42320,    543,  30158,    558,  11323,  24142,  72536,\n",
              "         23527,  58742,  36069,  25620,  81198,  15888,  16893,  10783,  17891,\n",
              "         10521,  10990,    542,  10517,  70426,    562,  11899,  10513,  43178,\n",
              "         28940,  49007,    543,  60092,  10122,    129,  15452,    543,  17891,\n",
              "         10521,  10990,    549,  10234,  32418,  60603,  11833,  10956,    558,\n",
              "         54697,  10364,  10296,    543,  79463,    570,  10746,  67196,  14614,\n",
              "           562,  86840, 104037,  12029,  10332,  90402,  10314,  10387,  10365,\n",
              "        109300,  17969,  13036,  10956,  33276,  41601,    555,  26245,  72459,\n",
              "         84435,  10191,  10868,  15811,    554,  10353,  10517,  23727,  72206,\n",
              "         79703,  98446,  10227,    543,  62516,  17891,  10521,  10990,    543,\n",
              "         39258,  26924,  26773,  10234,  85584,  17969,  13460,    561,  65674,\n",
              "         54274,    556,  51116,  10656,    570,  10746,  67196,  11579,  44225,\n",
              "         12500,  53117,  10122,  10439,  11833,  19900,  10191,  46920,  10205,\n",
              "         86691,  26773,    558,  27796,  15469,  38209,  10241,    543,  10247,\n",
              "         88312,  10332,  10783,  49964,  11833,    551,  72655,    551,  92190,\n",
              "         19120,    549,    558,  38659,  16616,  15266,  10205,  11921,  12999,\n",
              "         10191,  63726,    560,    555,  50751,  45760,  22048,  78999,  65764,\n",
              "           543,  10186,  34843,    549,  24052,  85854,  61782,  11833,  10122,\n",
              "         11921, 107631,  27889,    543, 100182,  10191,  51406,    562,  86840,\n",
              "        104037,  12029,    543,  26541,  92954,  10241,  22863,  30543,  16635,\n",
              "           543,  10193,  34843,    543,  67606,  11060,  10332,  11079,  47546,\n",
              "         70781,  12709,  60515,  10179, 109300,  17969,  13036,  10297,  28365,\n",
              "         62516,  10241,    549,  12709,  54288,  11613,  10297,  10413,  79703,\n",
              "         98446,  10292,  10234,  56762,  13713,  33054,  28171,  25269,  19676,\n",
              "        103229,  10752,  29065,  10122,  11279,  12118,  10387,  15597,  30158,\n",
              "         10191,    558,  32307,  16346,  10970,    543,  60092,  10122,    129,\n",
              "         15452,  75363,  10332,  88794,  42320,  12450,  28483,  37876,  11579,\n",
              "           551,  18510,  80465,  12743,  10513,  10122,    561,  16610,  70835,\n",
              "         26245,  36694,    561,  10674,  12686,  45760,  10842,    543,  46809,\n",
              "         26042,  54240,  78417, 103270,  19682,  10191,  12450,  28483,  37876,\n",
              "         14614,  13166,  57191,  16821,  10519,  76463,  10267,  10234,  85584,\n",
              "         17969,  13460,  79703,  98446,  10227,    541,  10933,  41543,  53869,\n",
              "         40124,  11718,  13166,  57191,  16821,  10519,  46809,  14995,  10267,\n",
              "         10332,  28354,  25556,  42500,  10191,  12634,  10513,  13713,  10387,\n",
              "         64371,    561,  16610,  70835, 100380,  10122,    558,  54697,  10227,\n",
              "         19544,  48613,  14655,  46070,  10852,    543,  38790,  28301,    561,\n",
              "         10674,  12686,  45760,  10277,  54841,  36694,  18481,  85584,  91945,\n",
              "         10191,  10332,  90402,  11055,  10387,  11303, 109300,  17969,  13036,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
              "             0,      0,      0,      0,      0,      0,      0,      0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUdV0dp5dCtr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je6kPJm6r8Ob"
      },
      "source": [
        "Меняем код под наши переменные. Уменьшаем размер батча, иначе в GPU коллаба не влезает"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPCpKWephTDp"
      },
      "source": [
        "from torch.utils.data import TensorDataset\r\n",
        "\r\n",
        "train_dataset = TensorDataset(X_train, X_train_masks, torch.tensor(y_train))\r\n",
        "val_dataset = TensorDataset(X_valid, X_valid_masks, torch.tensor(y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yia-MePAdCiQ"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \r\n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \r\n",
        "# size of 16 or 32.\r\n",
        "batch_size = 1\r\n",
        "\r\n",
        "# Create the DataLoaders for our training and validation sets.\r\n",
        "# We'll take training samples in random order. \r\n",
        "train_dataloader = DataLoader(\r\n",
        "            train_dataset,  # The training samples.\r\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\r\n",
        "            batch_size = batch_size # Trains with this batch size.\r\n",
        "        )\r\n",
        "\r\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\r\n",
        "validation_dataloader = DataLoader(\r\n",
        "            val_dataset, # The validation samples.\r\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\r\n",
        "            batch_size = batch_size # Evaluate with this batch size.\r\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLShUpNwdrKa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFYluUVwd8x5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_why4V82rpDI"
      },
      "source": [
        "Дальше алгоритм обучения оставляем, как в примере, только меняем модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot89Q3V0dq92",
        "outputId": "8da6e008-5122-4d2a-bb24-0e0b8ec4284f"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\r\n",
        "\r\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \r\n",
        "# linear classification layer on top. \r\n",
        "model = BertForSequenceClassification.from_pretrained(\r\n",
        "    #\"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\r\n",
        "    MODEL_NAME,\r\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\r\n",
        "                    # You can increase this for multi-class tasks.   \r\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\r\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\r\n",
        ")\r\n",
        "\r\n",
        "# Tell pytorch to run this model on the GPU.\r\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQVPoRHFebTR",
        "outputId": "b49c96ad-0bd2-4c38-db2d-2bdd40ba905a"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\r\n",
        "params = list(model.named_parameters())\r\n",
        "\r\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\r\n",
        "\r\n",
        "print('==== Embedding Layer ====\\n')\r\n",
        "\r\n",
        "for p in params[0:5]:\r\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\r\n",
        "\r\n",
        "print('\\n==== First Transformer ====\\n')\r\n",
        "\r\n",
        "for p in params[5:21]:\r\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\r\n",
        "\r\n",
        "print('\\n==== Output Layer ====\\n')\r\n",
        "\r\n",
        "for p in params[-4:]:\r\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir6aYGxIecTK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFUh-6APecD5"
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \r\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\r\n",
        "optimizer = AdamW(model.parameters(),\r\n",
        "                  #lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\r\n",
        "                  lr = 5e-7, # args.learning_rate - default is 5e-5, our notebook had 2e-5\r\n",
        "                  eps = 6e-8 # args.adam_epsilon  - default is 1e-8.\r\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-6nuq3AeglY"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "\r\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \r\n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\r\n",
        "# training data.\r\n",
        "epochs = 3\r\n",
        "\r\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \r\n",
        "# (Note that this is not the same as the number of training samples).\r\n",
        "total_steps = len(train_dataloader) * epochs\r\n",
        "\r\n",
        "# Create the learning rate scheduler.\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \r\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\r\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZB0gT8Mep7Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjHKdlhQejHA"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "# Function to calculate the accuracy of our predictions vs labels\r\n",
        "def flat_accuracy(preds, labels):\r\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf7MlMnbepGo"
      },
      "source": [
        "import time\r\n",
        "import datetime\r\n",
        "\r\n",
        "def format_time(elapsed):\r\n",
        "    '''\r\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\r\n",
        "    '''\r\n",
        "    # Round to the nearest second.\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # Format as hh:mm:ss\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-aWsG8Vev4j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z0hfQhPewcU",
        "outputId": "4d4527e3-eada-47a5-d595-752a15541781"
      },
      "source": [
        "import random\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# This training code is based on the `run_glue.py` script here:\r\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\r\n",
        "\r\n",
        "# Set the seed value all over the place to make this reproducible.\r\n",
        "seed_val =11\r\n",
        "\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)\r\n",
        "\r\n",
        "# We'll store a number of quantities such as training and validation loss, \r\n",
        "# validation accuracy, and timings.\r\n",
        "training_stats = []\r\n",
        "\r\n",
        "# Measure the total training time for the whole run.\r\n",
        "total_t0 = time.time()\r\n",
        "\r\n",
        "# For each epoch...\r\n",
        "for epoch_i in range(0, epochs):\r\n",
        "    \r\n",
        "    # ========================================\r\n",
        "    #               Training\r\n",
        "    # ========================================\r\n",
        "    \r\n",
        "    # Perform one full pass over the training set.\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n",
        "    print('Training...')\r\n",
        "\r\n",
        "    # Measure how long the training epoch takes.\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Reset the total loss for this epoch.\r\n",
        "    total_train_loss = 0\r\n",
        "\r\n",
        "    # Put the model into training mode. Don't be mislead--the call to \r\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\r\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\r\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    # For each batch of training data...\r\n",
        "    for step, batch in enumerate(train_dataloader):\r\n",
        "\r\n",
        "        # Progress update every 40 batches.\r\n",
        "        if step % 40 == 0 and not step == 0:\r\n",
        "            # Calculate elapsed time in minutes.\r\n",
        "            elapsed = format_time(time.time() - t0)\r\n",
        "            \r\n",
        "            # Report progress.\r\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\r\n",
        "\r\n",
        "        # Unpack this training batch from our dataloader. \r\n",
        "        #\r\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \r\n",
        "        # `to` method.\r\n",
        "        #\r\n",
        "        # `batch` contains three pytorch tensors:\r\n",
        "        #   [0]: input ids \r\n",
        "        #   [1]: attention masks\r\n",
        "        #   [2]: labels \r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device)\r\n",
        "\r\n",
        "        # Always clear any previously calculated gradients before performing a\r\n",
        "        # backward pass. PyTorch doesn't do this automatically because \r\n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \r\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\r\n",
        "        model.zero_grad()        \r\n",
        "\r\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\r\n",
        "        # The documentation for this `model` function is here: \r\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
        "        # It returns different numbers of parameters depending on what arguments\r\n",
        "        # arge given and what flags are set. For our useage here, it returns\r\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\r\n",
        "        # outputs prior to activation.\r\n",
        "        # loss, logits = model(b_input_ids, \r\n",
        "        #                      token_type_ids=None, \r\n",
        "        #                      attention_mask=b_input_mask, \r\n",
        "        #                      labels=b_labels)\r\n",
        "        output = model(b_input_ids, \r\n",
        "                        token_type_ids=None, \r\n",
        "                        attention_mask=b_input_mask, \r\n",
        "                        labels=b_labels)\r\n",
        "        loss = output.loss\r\n",
        "        logits = output.logits\r\n",
        "\r\n",
        "\r\n",
        "        # Accumulate the training loss over all of the batches so that we can\r\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\r\n",
        "        # single value; the `.item()` function just returns the Python value \r\n",
        "        # from the tensor.\r\n",
        "        total_train_loss += loss.item()\r\n",
        "\r\n",
        "        # Perform a backward pass to calculate the gradients.\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Clip the norm of the gradients to 1.0.\r\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "        # Update parameters and take a step using the computed gradient.\r\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\r\n",
        "        # modified based on their gradients, the learning rate, etc.\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # Update the learning rate.\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "    # Calculate the average loss over all of the batches.\r\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \r\n",
        "    \r\n",
        "    # Measure how long this epoch took.\r\n",
        "    training_time = format_time(time.time() - t0)\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\r\n",
        "        \r\n",
        "    # ========================================\r\n",
        "    #               Validation\r\n",
        "    # ========================================\r\n",
        "    # After the completion of each training epoch, measure our performance on\r\n",
        "    # our validation set.\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"Running Validation...\")\r\n",
        "\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\r\n",
        "    # during evaluation.\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    # Tracking variables \r\n",
        "    total_eval_accuracy = 0\r\n",
        "    total_eval_loss = 0\r\n",
        "    nb_eval_steps = 0\r\n",
        "\r\n",
        "    # Evaluate data for one epoch\r\n",
        "    for batch in validation_dataloader:\r\n",
        "        \r\n",
        "        # Unpack this training batch from our dataloader. \r\n",
        "        #\r\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \r\n",
        "        # the `to` method.\r\n",
        "        #\r\n",
        "        # `batch` contains three pytorch tensors:\r\n",
        "        #   [0]: input ids \r\n",
        "        #   [1]: attention masks\r\n",
        "        #   [2]: labels \r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device)\r\n",
        "        \r\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\r\n",
        "        # the forward pass, since this is only needed for backprop (training).\r\n",
        "        with torch.no_grad():        \r\n",
        "\r\n",
        "            # Forward pass, calculate logit predictions.\r\n",
        "            # token_type_ids is the same as the \"segment ids\", which \r\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\r\n",
        "            # The documentation for this `model` function is here: \r\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\r\n",
        "            # values prior to applying an activation function like the softmax.\r\n",
        "\r\n",
        "            # (loss, logits) = model(b_input_ids, \r\n",
        "            #                        token_type_ids=None, \r\n",
        "            #                        attention_mask=b_input_mask,\r\n",
        "            #                        labels=b_labels)\r\n",
        "            output = model(b_input_ids, \r\n",
        "                           token_type_ids=None, \r\n",
        "                           attention_mask=b_input_mask, \r\n",
        "                           labels=b_labels)\r\n",
        "            loss = output.loss\r\n",
        "            logits = output.logits\r\n",
        "            \r\n",
        "        # Accumulate the validation loss.\r\n",
        "        total_eval_loss += loss.item()\r\n",
        "\r\n",
        "        # Move logits and labels to CPU\r\n",
        "        logits = logits.detach().cpu().numpy()\r\n",
        "        label_ids = b_labels.to('cpu').numpy()\r\n",
        "\r\n",
        "        # Calculate the accuracy for this batch of test sentences, and\r\n",
        "        # accumulate it over all batches.\r\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\r\n",
        "        \r\n",
        "\r\n",
        "    # Report the final accuracy for this validation run.\r\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\r\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\r\n",
        "\r\n",
        "    # Calculate the average loss over all of the batches.\r\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\r\n",
        "    \r\n",
        "    # Measure how long the validation run took.\r\n",
        "    validation_time = format_time(time.time() - t0)\r\n",
        "    \r\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\r\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\r\n",
        "\r\n",
        "    # Record all statistics from this epoch.\r\n",
        "    training_stats.append(\r\n",
        "        {\r\n",
        "            'epoch': epoch_i + 1,\r\n",
        "            'Training Loss': avg_train_loss,\r\n",
        "            'Valid. Loss': avg_val_loss,\r\n",
        "            'Valid. Accur.': avg_val_accuracy,\r\n",
        "            'Training Time': training_time,\r\n",
        "            'Validation Time': validation_time\r\n",
        "        }\r\n",
        "    )\r\n",
        "\r\n",
        "print(\"\")\r\n",
        "print(\"Training complete!\")\r\n",
        "\r\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  11,950.    Elapsed: 0:00:07.\n",
            "  Batch    80  of  11,950.    Elapsed: 0:00:14.\n",
            "  Batch   120  of  11,950.    Elapsed: 0:00:22.\n",
            "  Batch   160  of  11,950.    Elapsed: 0:00:29.\n",
            "  Batch   200  of  11,950.    Elapsed: 0:00:36.\n",
            "  Batch   240  of  11,950.    Elapsed: 0:00:43.\n",
            "  Batch   280  of  11,950.    Elapsed: 0:00:50.\n",
            "  Batch   320  of  11,950.    Elapsed: 0:00:57.\n",
            "  Batch   360  of  11,950.    Elapsed: 0:01:04.\n",
            "  Batch   400  of  11,950.    Elapsed: 0:01:11.\n",
            "  Batch   440  of  11,950.    Elapsed: 0:01:18.\n",
            "  Batch   480  of  11,950.    Elapsed: 0:01:25.\n",
            "  Batch   520  of  11,950.    Elapsed: 0:01:32.\n",
            "  Batch   560  of  11,950.    Elapsed: 0:01:39.\n",
            "  Batch   600  of  11,950.    Elapsed: 0:01:46.\n",
            "  Batch   640  of  11,950.    Elapsed: 0:01:53.\n",
            "  Batch   680  of  11,950.    Elapsed: 0:02:00.\n",
            "  Batch   720  of  11,950.    Elapsed: 0:02:07.\n",
            "  Batch   760  of  11,950.    Elapsed: 0:02:14.\n",
            "  Batch   800  of  11,950.    Elapsed: 0:02:21.\n",
            "  Batch   840  of  11,950.    Elapsed: 0:02:28.\n",
            "  Batch   880  of  11,950.    Elapsed: 0:02:35.\n",
            "  Batch   920  of  11,950.    Elapsed: 0:02:42.\n",
            "  Batch   960  of  11,950.    Elapsed: 0:02:49.\n",
            "  Batch 1,000  of  11,950.    Elapsed: 0:02:56.\n",
            "  Batch 1,040  of  11,950.    Elapsed: 0:03:03.\n",
            "  Batch 1,080  of  11,950.    Elapsed: 0:03:10.\n",
            "  Batch 1,120  of  11,950.    Elapsed: 0:03:17.\n",
            "  Batch 1,160  of  11,950.    Elapsed: 0:03:24.\n",
            "  Batch 1,200  of  11,950.    Elapsed: 0:03:31.\n",
            "  Batch 1,240  of  11,950.    Elapsed: 0:03:39.\n",
            "  Batch 1,280  of  11,950.    Elapsed: 0:03:46.\n",
            "  Batch 1,320  of  11,950.    Elapsed: 0:03:53.\n",
            "  Batch 1,360  of  11,950.    Elapsed: 0:04:00.\n",
            "  Batch 1,400  of  11,950.    Elapsed: 0:04:07.\n",
            "  Batch 1,440  of  11,950.    Elapsed: 0:04:14.\n",
            "  Batch 1,480  of  11,950.    Elapsed: 0:04:21.\n",
            "  Batch 1,520  of  11,950.    Elapsed: 0:04:28.\n",
            "  Batch 1,560  of  11,950.    Elapsed: 0:04:35.\n",
            "  Batch 1,600  of  11,950.    Elapsed: 0:04:42.\n",
            "  Batch 1,640  of  11,950.    Elapsed: 0:04:49.\n",
            "  Batch 1,680  of  11,950.    Elapsed: 0:04:56.\n",
            "  Batch 1,720  of  11,950.    Elapsed: 0:05:03.\n",
            "  Batch 1,760  of  11,950.    Elapsed: 0:05:10.\n",
            "  Batch 1,800  of  11,950.    Elapsed: 0:05:17.\n",
            "  Batch 1,840  of  11,950.    Elapsed: 0:05:24.\n",
            "  Batch 1,880  of  11,950.    Elapsed: 0:05:31.\n",
            "  Batch 1,920  of  11,950.    Elapsed: 0:05:38.\n",
            "  Batch 1,960  of  11,950.    Elapsed: 0:05:45.\n",
            "  Batch 2,000  of  11,950.    Elapsed: 0:05:52.\n",
            "  Batch 2,040  of  11,950.    Elapsed: 0:05:59.\n",
            "  Batch 2,080  of  11,950.    Elapsed: 0:06:06.\n",
            "  Batch 2,120  of  11,950.    Elapsed: 0:06:13.\n",
            "  Batch 2,160  of  11,950.    Elapsed: 0:06:20.\n",
            "  Batch 2,200  of  11,950.    Elapsed: 0:06:27.\n",
            "  Batch 2,240  of  11,950.    Elapsed: 0:06:34.\n",
            "  Batch 2,280  of  11,950.    Elapsed: 0:06:41.\n",
            "  Batch 2,320  of  11,950.    Elapsed: 0:06:48.\n",
            "  Batch 2,360  of  11,950.    Elapsed: 0:06:55.\n",
            "  Batch 2,400  of  11,950.    Elapsed: 0:07:02.\n",
            "  Batch 2,440  of  11,950.    Elapsed: 0:07:09.\n",
            "  Batch 2,480  of  11,950.    Elapsed: 0:07:16.\n",
            "  Batch 2,520  of  11,950.    Elapsed: 0:07:23.\n",
            "  Batch 2,560  of  11,950.    Elapsed: 0:07:30.\n",
            "  Batch 2,600  of  11,950.    Elapsed: 0:07:37.\n",
            "  Batch 2,640  of  11,950.    Elapsed: 0:07:44.\n",
            "  Batch 2,680  of  11,950.    Elapsed: 0:07:51.\n",
            "  Batch 2,720  of  11,950.    Elapsed: 0:07:58.\n",
            "  Batch 2,760  of  11,950.    Elapsed: 0:08:05.\n",
            "  Batch 2,800  of  11,950.    Elapsed: 0:08:12.\n",
            "  Batch 2,840  of  11,950.    Elapsed: 0:08:19.\n",
            "  Batch 2,880  of  11,950.    Elapsed: 0:08:26.\n",
            "  Batch 2,920  of  11,950.    Elapsed: 0:08:33.\n",
            "  Batch 2,960  of  11,950.    Elapsed: 0:08:40.\n",
            "  Batch 3,000  of  11,950.    Elapsed: 0:08:47.\n",
            "  Batch 3,040  of  11,950.    Elapsed: 0:08:54.\n",
            "  Batch 3,080  of  11,950.    Elapsed: 0:09:01.\n",
            "  Batch 3,120  of  11,950.    Elapsed: 0:09:08.\n",
            "  Batch 3,160  of  11,950.    Elapsed: 0:09:15.\n",
            "  Batch 3,200  of  11,950.    Elapsed: 0:09:22.\n",
            "  Batch 3,240  of  11,950.    Elapsed: 0:09:29.\n",
            "  Batch 3,280  of  11,950.    Elapsed: 0:09:36.\n",
            "  Batch 3,320  of  11,950.    Elapsed: 0:09:43.\n",
            "  Batch 3,360  of  11,950.    Elapsed: 0:09:50.\n",
            "  Batch 3,400  of  11,950.    Elapsed: 0:09:57.\n",
            "  Batch 3,440  of  11,950.    Elapsed: 0:10:04.\n",
            "  Batch 3,480  of  11,950.    Elapsed: 0:10:11.\n",
            "  Batch 3,520  of  11,950.    Elapsed: 0:10:18.\n",
            "  Batch 3,560  of  11,950.    Elapsed: 0:10:24.\n",
            "  Batch 3,600  of  11,950.    Elapsed: 0:10:31.\n",
            "  Batch 3,640  of  11,950.    Elapsed: 0:10:38.\n",
            "  Batch 3,680  of  11,950.    Elapsed: 0:10:45.\n",
            "  Batch 3,720  of  11,950.    Elapsed: 0:10:52.\n",
            "  Batch 3,760  of  11,950.    Elapsed: 0:10:59.\n",
            "  Batch 3,800  of  11,950.    Elapsed: 0:11:06.\n",
            "  Batch 3,840  of  11,950.    Elapsed: 0:11:13.\n",
            "  Batch 3,880  of  11,950.    Elapsed: 0:11:20.\n",
            "  Batch 3,920  of  11,950.    Elapsed: 0:11:27.\n",
            "  Batch 3,960  of  11,950.    Elapsed: 0:11:34.\n",
            "  Batch 4,000  of  11,950.    Elapsed: 0:11:41.\n",
            "  Batch 4,040  of  11,950.    Elapsed: 0:11:48.\n",
            "  Batch 4,080  of  11,950.    Elapsed: 0:11:55.\n",
            "  Batch 4,120  of  11,950.    Elapsed: 0:12:02.\n",
            "  Batch 4,160  of  11,950.    Elapsed: 0:12:09.\n",
            "  Batch 4,200  of  11,950.    Elapsed: 0:12:16.\n",
            "  Batch 4,240  of  11,950.    Elapsed: 0:12:23.\n",
            "  Batch 4,280  of  11,950.    Elapsed: 0:12:30.\n",
            "  Batch 4,320  of  11,950.    Elapsed: 0:12:37.\n",
            "  Batch 4,360  of  11,950.    Elapsed: 0:12:43.\n",
            "  Batch 4,400  of  11,950.    Elapsed: 0:12:50.\n",
            "  Batch 4,440  of  11,950.    Elapsed: 0:12:57.\n",
            "  Batch 4,480  of  11,950.    Elapsed: 0:13:04.\n",
            "  Batch 4,520  of  11,950.    Elapsed: 0:13:11.\n",
            "  Batch 4,560  of  11,950.    Elapsed: 0:13:18.\n",
            "  Batch 4,600  of  11,950.    Elapsed: 0:13:25.\n",
            "  Batch 4,640  of  11,950.    Elapsed: 0:13:32.\n",
            "  Batch 4,680  of  11,950.    Elapsed: 0:13:39.\n",
            "  Batch 4,720  of  11,950.    Elapsed: 0:13:46.\n",
            "  Batch 4,760  of  11,950.    Elapsed: 0:13:53.\n",
            "  Batch 4,800  of  11,950.    Elapsed: 0:14:00.\n",
            "  Batch 4,840  of  11,950.    Elapsed: 0:14:07.\n",
            "  Batch 4,880  of  11,950.    Elapsed: 0:14:14.\n",
            "  Batch 4,920  of  11,950.    Elapsed: 0:14:21.\n",
            "  Batch 4,960  of  11,950.    Elapsed: 0:14:28.\n",
            "  Batch 5,000  of  11,950.    Elapsed: 0:14:34.\n",
            "  Batch 5,040  of  11,950.    Elapsed: 0:14:41.\n",
            "  Batch 5,080  of  11,950.    Elapsed: 0:14:48.\n",
            "  Batch 5,120  of  11,950.    Elapsed: 0:14:55.\n",
            "  Batch 5,160  of  11,950.    Elapsed: 0:15:02.\n",
            "  Batch 5,200  of  11,950.    Elapsed: 0:15:09.\n",
            "  Batch 5,240  of  11,950.    Elapsed: 0:15:16.\n",
            "  Batch 5,280  of  11,950.    Elapsed: 0:15:23.\n",
            "  Batch 5,320  of  11,950.    Elapsed: 0:15:30.\n",
            "  Batch 5,360  of  11,950.    Elapsed: 0:15:37.\n",
            "  Batch 5,400  of  11,950.    Elapsed: 0:15:44.\n",
            "  Batch 5,440  of  11,950.    Elapsed: 0:15:51.\n",
            "  Batch 5,480  of  11,950.    Elapsed: 0:15:58.\n",
            "  Batch 5,520  of  11,950.    Elapsed: 0:16:04.\n",
            "  Batch 5,560  of  11,950.    Elapsed: 0:16:11.\n",
            "  Batch 5,600  of  11,950.    Elapsed: 0:16:18.\n",
            "  Batch 5,640  of  11,950.    Elapsed: 0:16:25.\n",
            "  Batch 5,680  of  11,950.    Elapsed: 0:16:32.\n",
            "  Batch 5,720  of  11,950.    Elapsed: 0:16:39.\n",
            "  Batch 5,760  of  11,950.    Elapsed: 0:16:46.\n",
            "  Batch 5,800  of  11,950.    Elapsed: 0:16:53.\n",
            "  Batch 5,840  of  11,950.    Elapsed: 0:17:00.\n",
            "  Batch 5,880  of  11,950.    Elapsed: 0:17:07.\n",
            "  Batch 5,920  of  11,950.    Elapsed: 0:17:14.\n",
            "  Batch 5,960  of  11,950.    Elapsed: 0:17:21.\n",
            "  Batch 6,000  of  11,950.    Elapsed: 0:17:28.\n",
            "  Batch 6,040  of  11,950.    Elapsed: 0:17:35.\n",
            "  Batch 6,080  of  11,950.    Elapsed: 0:17:42.\n",
            "  Batch 6,120  of  11,950.    Elapsed: 0:17:49.\n",
            "  Batch 6,160  of  11,950.    Elapsed: 0:17:55.\n",
            "  Batch 6,200  of  11,950.    Elapsed: 0:18:02.\n",
            "  Batch 6,240  of  11,950.    Elapsed: 0:18:09.\n",
            "  Batch 6,280  of  11,950.    Elapsed: 0:18:16.\n",
            "  Batch 6,320  of  11,950.    Elapsed: 0:18:23.\n",
            "  Batch 6,360  of  11,950.    Elapsed: 0:18:30.\n",
            "  Batch 6,400  of  11,950.    Elapsed: 0:18:37.\n",
            "  Batch 6,440  of  11,950.    Elapsed: 0:18:44.\n",
            "  Batch 6,480  of  11,950.    Elapsed: 0:18:51.\n",
            "  Batch 6,520  of  11,950.    Elapsed: 0:18:58.\n",
            "  Batch 6,560  of  11,950.    Elapsed: 0:19:05.\n",
            "  Batch 6,600  of  11,950.    Elapsed: 0:19:12.\n",
            "  Batch 6,640  of  11,950.    Elapsed: 0:19:19.\n",
            "  Batch 6,680  of  11,950.    Elapsed: 0:19:26.\n",
            "  Batch 6,720  of  11,950.    Elapsed: 0:19:33.\n",
            "  Batch 6,760  of  11,950.    Elapsed: 0:19:40.\n",
            "  Batch 6,800  of  11,950.    Elapsed: 0:19:47.\n",
            "  Batch 6,840  of  11,950.    Elapsed: 0:19:54.\n",
            "  Batch 6,880  of  11,950.    Elapsed: 0:20:01.\n",
            "  Batch 6,920  of  11,950.    Elapsed: 0:20:08.\n",
            "  Batch 6,960  of  11,950.    Elapsed: 0:20:15.\n",
            "  Batch 7,000  of  11,950.    Elapsed: 0:20:22.\n",
            "  Batch 7,040  of  11,950.    Elapsed: 0:20:29.\n",
            "  Batch 7,080  of  11,950.    Elapsed: 0:20:36.\n",
            "  Batch 7,120  of  11,950.    Elapsed: 0:20:43.\n",
            "  Batch 7,160  of  11,950.    Elapsed: 0:20:50.\n",
            "  Batch 7,200  of  11,950.    Elapsed: 0:20:57.\n",
            "  Batch 7,240  of  11,950.    Elapsed: 0:21:04.\n",
            "  Batch 7,280  of  11,950.    Elapsed: 0:21:11.\n",
            "  Batch 7,320  of  11,950.    Elapsed: 0:21:17.\n",
            "  Batch 7,360  of  11,950.    Elapsed: 0:21:24.\n",
            "  Batch 7,400  of  11,950.    Elapsed: 0:21:31.\n",
            "  Batch 7,440  of  11,950.    Elapsed: 0:21:38.\n",
            "  Batch 7,480  of  11,950.    Elapsed: 0:21:45.\n",
            "  Batch 7,520  of  11,950.    Elapsed: 0:21:52.\n",
            "  Batch 7,560  of  11,950.    Elapsed: 0:21:59.\n",
            "  Batch 7,600  of  11,950.    Elapsed: 0:22:06.\n",
            "  Batch 7,640  of  11,950.    Elapsed: 0:22:13.\n",
            "  Batch 7,680  of  11,950.    Elapsed: 0:22:20.\n",
            "  Batch 7,720  of  11,950.    Elapsed: 0:22:27.\n",
            "  Batch 7,760  of  11,950.    Elapsed: 0:22:34.\n",
            "  Batch 7,800  of  11,950.    Elapsed: 0:22:41.\n",
            "  Batch 7,840  of  11,950.    Elapsed: 0:22:48.\n",
            "  Batch 7,880  of  11,950.    Elapsed: 0:22:54.\n",
            "  Batch 7,920  of  11,950.    Elapsed: 0:23:01.\n",
            "  Batch 7,960  of  11,950.    Elapsed: 0:23:08.\n",
            "  Batch 8,000  of  11,950.    Elapsed: 0:23:15.\n",
            "  Batch 8,040  of  11,950.    Elapsed: 0:23:22.\n",
            "  Batch 8,080  of  11,950.    Elapsed: 0:23:29.\n",
            "  Batch 8,120  of  11,950.    Elapsed: 0:23:36.\n",
            "  Batch 8,160  of  11,950.    Elapsed: 0:23:43.\n",
            "  Batch 8,200  of  11,950.    Elapsed: 0:23:50.\n",
            "  Batch 8,240  of  11,950.    Elapsed: 0:23:57.\n",
            "  Batch 8,280  of  11,950.    Elapsed: 0:24:04.\n",
            "  Batch 8,320  of  11,950.    Elapsed: 0:24:11.\n",
            "  Batch 8,360  of  11,950.    Elapsed: 0:24:18.\n",
            "  Batch 8,400  of  11,950.    Elapsed: 0:24:25.\n",
            "  Batch 8,440  of  11,950.    Elapsed: 0:24:32.\n",
            "  Batch 8,480  of  11,950.    Elapsed: 0:24:38.\n",
            "  Batch 8,520  of  11,950.    Elapsed: 0:24:45.\n",
            "  Batch 8,560  of  11,950.    Elapsed: 0:24:52.\n",
            "  Batch 8,600  of  11,950.    Elapsed: 0:24:59.\n",
            "  Batch 8,640  of  11,950.    Elapsed: 0:25:06.\n",
            "  Batch 8,680  of  11,950.    Elapsed: 0:25:13.\n",
            "  Batch 8,720  of  11,950.    Elapsed: 0:25:20.\n",
            "  Batch 8,760  of  11,950.    Elapsed: 0:25:27.\n",
            "  Batch 8,800  of  11,950.    Elapsed: 0:25:34.\n",
            "  Batch 8,840  of  11,950.    Elapsed: 0:25:41.\n",
            "  Batch 8,880  of  11,950.    Elapsed: 0:25:48.\n",
            "  Batch 8,920  of  11,950.    Elapsed: 0:25:55.\n",
            "  Batch 8,960  of  11,950.    Elapsed: 0:26:02.\n",
            "  Batch 9,000  of  11,950.    Elapsed: 0:26:09.\n",
            "  Batch 9,040  of  11,950.    Elapsed: 0:26:16.\n",
            "  Batch 9,080  of  11,950.    Elapsed: 0:26:23.\n",
            "  Batch 9,120  of  11,950.    Elapsed: 0:26:30.\n",
            "  Batch 9,160  of  11,950.    Elapsed: 0:26:36.\n",
            "  Batch 9,200  of  11,950.    Elapsed: 0:26:43.\n",
            "  Batch 9,240  of  11,950.    Elapsed: 0:26:50.\n",
            "  Batch 9,280  of  11,950.    Elapsed: 0:26:57.\n",
            "  Batch 9,320  of  11,950.    Elapsed: 0:27:04.\n",
            "  Batch 9,360  of  11,950.    Elapsed: 0:27:11.\n",
            "  Batch 9,400  of  11,950.    Elapsed: 0:27:18.\n",
            "  Batch 9,440  of  11,950.    Elapsed: 0:27:25.\n",
            "  Batch 9,480  of  11,950.    Elapsed: 0:27:32.\n",
            "  Batch 9,520  of  11,950.    Elapsed: 0:27:39.\n",
            "  Batch 9,560  of  11,950.    Elapsed: 0:27:46.\n",
            "  Batch 9,600  of  11,950.    Elapsed: 0:27:53.\n",
            "  Batch 9,640  of  11,950.    Elapsed: 0:28:00.\n",
            "  Batch 9,680  of  11,950.    Elapsed: 0:28:07.\n",
            "  Batch 9,720  of  11,950.    Elapsed: 0:28:14.\n",
            "  Batch 9,760  of  11,950.    Elapsed: 0:28:21.\n",
            "  Batch 9,800  of  11,950.    Elapsed: 0:28:28.\n",
            "  Batch 9,840  of  11,950.    Elapsed: 0:28:35.\n",
            "  Batch 9,880  of  11,950.    Elapsed: 0:28:42.\n",
            "  Batch 9,920  of  11,950.    Elapsed: 0:28:49.\n",
            "  Batch 9,960  of  11,950.    Elapsed: 0:28:56.\n",
            "  Batch 10,000  of  11,950.    Elapsed: 0:29:03.\n",
            "  Batch 10,040  of  11,950.    Elapsed: 0:29:10.\n",
            "  Batch 10,080  of  11,950.    Elapsed: 0:29:17.\n",
            "  Batch 10,120  of  11,950.    Elapsed: 0:29:24.\n",
            "  Batch 10,160  of  11,950.    Elapsed: 0:29:31.\n",
            "  Batch 10,200  of  11,950.    Elapsed: 0:29:38.\n",
            "  Batch 10,240  of  11,950.    Elapsed: 0:29:44.\n",
            "  Batch 10,280  of  11,950.    Elapsed: 0:29:51.\n",
            "  Batch 10,320  of  11,950.    Elapsed: 0:29:58.\n",
            "  Batch 10,360  of  11,950.    Elapsed: 0:30:05.\n",
            "  Batch 10,400  of  11,950.    Elapsed: 0:30:12.\n",
            "  Batch 10,440  of  11,950.    Elapsed: 0:30:19.\n",
            "  Batch 10,480  of  11,950.    Elapsed: 0:30:26.\n",
            "  Batch 10,520  of  11,950.    Elapsed: 0:30:33.\n",
            "  Batch 10,560  of  11,950.    Elapsed: 0:30:40.\n",
            "  Batch 10,600  of  11,950.    Elapsed: 0:30:47.\n",
            "  Batch 10,640  of  11,950.    Elapsed: 0:30:54.\n",
            "  Batch 10,680  of  11,950.    Elapsed: 0:31:01.\n",
            "  Batch 10,720  of  11,950.    Elapsed: 0:31:08.\n",
            "  Batch 10,760  of  11,950.    Elapsed: 0:31:15.\n",
            "  Batch 10,800  of  11,950.    Elapsed: 0:31:22.\n",
            "  Batch 10,840  of  11,950.    Elapsed: 0:31:29.\n",
            "  Batch 10,880  of  11,950.    Elapsed: 0:31:36.\n",
            "  Batch 10,920  of  11,950.    Elapsed: 0:31:43.\n",
            "  Batch 10,960  of  11,950.    Elapsed: 0:31:50.\n",
            "  Batch 11,000  of  11,950.    Elapsed: 0:31:57.\n",
            "  Batch 11,040  of  11,950.    Elapsed: 0:32:04.\n",
            "  Batch 11,080  of  11,950.    Elapsed: 0:32:11.\n",
            "  Batch 11,120  of  11,950.    Elapsed: 0:32:18.\n",
            "  Batch 11,160  of  11,950.    Elapsed: 0:32:25.\n",
            "  Batch 11,200  of  11,950.    Elapsed: 0:32:32.\n",
            "  Batch 11,240  of  11,950.    Elapsed: 0:32:39.\n",
            "  Batch 11,280  of  11,950.    Elapsed: 0:32:46.\n",
            "  Batch 11,320  of  11,950.    Elapsed: 0:32:53.\n",
            "  Batch 11,360  of  11,950.    Elapsed: 0:33:00.\n",
            "  Batch 11,400  of  11,950.    Elapsed: 0:33:07.\n",
            "  Batch 11,440  of  11,950.    Elapsed: 0:33:14.\n",
            "  Batch 11,480  of  11,950.    Elapsed: 0:33:20.\n",
            "  Batch 11,520  of  11,950.    Elapsed: 0:33:27.\n",
            "  Batch 11,560  of  11,950.    Elapsed: 0:33:34.\n",
            "  Batch 11,600  of  11,950.    Elapsed: 0:33:41.\n",
            "  Batch 11,640  of  11,950.    Elapsed: 0:33:48.\n",
            "  Batch 11,680  of  11,950.    Elapsed: 0:33:55.\n",
            "  Batch 11,720  of  11,950.    Elapsed: 0:34:02.\n",
            "  Batch 11,760  of  11,950.    Elapsed: 0:34:09.\n",
            "  Batch 11,800  of  11,950.    Elapsed: 0:34:16.\n",
            "  Batch 11,840  of  11,950.    Elapsed: 0:34:23.\n",
            "  Batch 11,880  of  11,950.    Elapsed: 0:34:30.\n",
            "  Batch 11,920  of  11,950.    Elapsed: 0:34:37.\n",
            "\n",
            "  Average training loss: 1.44\n",
            "  Training epcoh took: 0:34:42\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.39\n",
            "  Validation took: 0:01:30\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  11,950.    Elapsed: 0:00:07.\n",
            "  Batch    80  of  11,950.    Elapsed: 0:00:14.\n",
            "  Batch   120  of  11,950.    Elapsed: 0:00:21.\n",
            "  Batch   160  of  11,950.    Elapsed: 0:00:28.\n",
            "  Batch   200  of  11,950.    Elapsed: 0:00:35.\n",
            "  Batch   240  of  11,950.    Elapsed: 0:00:42.\n",
            "  Batch   280  of  11,950.    Elapsed: 0:00:49.\n",
            "  Batch   320  of  11,950.    Elapsed: 0:00:56.\n",
            "  Batch   360  of  11,950.    Elapsed: 0:01:03.\n",
            "  Batch   400  of  11,950.    Elapsed: 0:01:10.\n",
            "  Batch   440  of  11,950.    Elapsed: 0:01:17.\n",
            "  Batch   480  of  11,950.    Elapsed: 0:01:24.\n",
            "  Batch   520  of  11,950.    Elapsed: 0:01:31.\n",
            "  Batch   560  of  11,950.    Elapsed: 0:01:38.\n",
            "  Batch   600  of  11,950.    Elapsed: 0:01:45.\n",
            "  Batch   640  of  11,950.    Elapsed: 0:01:52.\n",
            "  Batch   680  of  11,950.    Elapsed: 0:01:59.\n",
            "  Batch   720  of  11,950.    Elapsed: 0:02:06.\n",
            "  Batch   760  of  11,950.    Elapsed: 0:02:13.\n",
            "  Batch   800  of  11,950.    Elapsed: 0:02:20.\n",
            "  Batch   840  of  11,950.    Elapsed: 0:02:27.\n",
            "  Batch   880  of  11,950.    Elapsed: 0:02:34.\n",
            "  Batch   920  of  11,950.    Elapsed: 0:02:40.\n",
            "  Batch   960  of  11,950.    Elapsed: 0:02:47.\n",
            "  Batch 1,000  of  11,950.    Elapsed: 0:02:54.\n",
            "  Batch 1,040  of  11,950.    Elapsed: 0:03:01.\n",
            "  Batch 1,080  of  11,950.    Elapsed: 0:03:08.\n",
            "  Batch 1,120  of  11,950.    Elapsed: 0:03:15.\n",
            "  Batch 1,160  of  11,950.    Elapsed: 0:03:22.\n",
            "  Batch 1,200  of  11,950.    Elapsed: 0:03:29.\n",
            "  Batch 1,240  of  11,950.    Elapsed: 0:03:36.\n",
            "  Batch 1,280  of  11,950.    Elapsed: 0:03:43.\n",
            "  Batch 1,320  of  11,950.    Elapsed: 0:03:50.\n",
            "  Batch 1,360  of  11,950.    Elapsed: 0:03:57.\n",
            "  Batch 1,400  of  11,950.    Elapsed: 0:04:04.\n",
            "  Batch 1,440  of  11,950.    Elapsed: 0:04:11.\n",
            "  Batch 1,480  of  11,950.    Elapsed: 0:04:18.\n",
            "  Batch 1,520  of  11,950.    Elapsed: 0:04:25.\n",
            "  Batch 1,560  of  11,950.    Elapsed: 0:04:32.\n",
            "  Batch 1,600  of  11,950.    Elapsed: 0:04:39.\n",
            "  Batch 1,640  of  11,950.    Elapsed: 0:04:46.\n",
            "  Batch 1,680  of  11,950.    Elapsed: 0:04:53.\n",
            "  Batch 1,720  of  11,950.    Elapsed: 0:05:00.\n",
            "  Batch 1,760  of  11,950.    Elapsed: 0:05:07.\n",
            "  Batch 1,800  of  11,950.    Elapsed: 0:05:14.\n",
            "  Batch 1,840  of  11,950.    Elapsed: 0:05:21.\n",
            "  Batch 1,880  of  11,950.    Elapsed: 0:05:28.\n",
            "  Batch 1,920  of  11,950.    Elapsed: 0:05:35.\n",
            "  Batch 1,960  of  11,950.    Elapsed: 0:05:42.\n",
            "  Batch 2,000  of  11,950.    Elapsed: 0:05:49.\n",
            "  Batch 2,040  of  11,950.    Elapsed: 0:05:56.\n",
            "  Batch 2,080  of  11,950.    Elapsed: 0:06:03.\n",
            "  Batch 2,120  of  11,950.    Elapsed: 0:06:10.\n",
            "  Batch 2,160  of  11,950.    Elapsed: 0:06:17.\n",
            "  Batch 2,200  of  11,950.    Elapsed: 0:06:23.\n",
            "  Batch 2,240  of  11,950.    Elapsed: 0:06:30.\n",
            "  Batch 2,280  of  11,950.    Elapsed: 0:06:37.\n",
            "  Batch 2,320  of  11,950.    Elapsed: 0:06:44.\n",
            "  Batch 2,360  of  11,950.    Elapsed: 0:06:51.\n",
            "  Batch 2,400  of  11,950.    Elapsed: 0:06:58.\n",
            "  Batch 2,440  of  11,950.    Elapsed: 0:07:05.\n",
            "  Batch 2,480  of  11,950.    Elapsed: 0:07:12.\n",
            "  Batch 2,520  of  11,950.    Elapsed: 0:07:19.\n",
            "  Batch 2,560  of  11,950.    Elapsed: 0:07:26.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7-izrg4khoq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8-4r6wb-ZPZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWsEpKaEhV_e"
      },
      "source": [
        "Строим красивые графики"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-tdYIy1g4W1"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "# Display floats with two decimal places.\r\n",
        "pd.set_option('precision', 2)\r\n",
        "\r\n",
        "# Create a DataFrame from our training statistics.\r\n",
        "df_stats = pd.DataFrame(data=training_stats)\r\n",
        "\r\n",
        "# Use the 'epoch' as the row index.\r\n",
        "df_stats = df_stats.set_index('epoch')\r\n",
        "\r\n",
        "# A hack to force the column headers to wrap.\r\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\r\n",
        "\r\n",
        "# Display the table.\r\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPjdW6DLg8uQ"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "% matplotlib inline\r\n",
        "\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "# Use plot styling from seaborn.\r\n",
        "sns.set(style='darkgrid')\r\n",
        "\r\n",
        "# Increase the plot size and font size.\r\n",
        "sns.set(font_scale=1.5)\r\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\r\n",
        "\r\n",
        "# Plot the learning curve.\r\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\r\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\r\n",
        "\r\n",
        "# Label the plot.\r\n",
        "plt.title(\"Training & Validation Loss\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.legend()\r\n",
        "plt.xticks([1, 2, 3, 4])\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfoFOz3Ig9yP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JepkgAIgg9la"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5-x7LhmnRR_"
      },
      "source": [
        "Делаем предсказание и сохраняем файл с submission'ом"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNXVVE9R5BN8"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\r\n",
        "input_ids = []\r\n",
        "attention_masks = []\r\n",
        "\r\n",
        "# For every sentence...\r\n",
        "for sent in X_test:\r\n",
        "    # `encode_plus` will:\r\n",
        "    #   (1) Tokenize the sentence.\r\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\r\n",
        "    #   (3) Append the `[SEP]` token to the end.\r\n",
        "    #   (4) Map tokens to their IDs.\r\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\r\n",
        "    #   (6) Create attention masks for [PAD] tokens.\r\n",
        "    encoded_dict = tokenizer.encode_plus(\r\n",
        "                        sent,                      # Sentence to encode.\r\n",
        "                        add_special_tokens = False, # Add '[CLS]' and '[SEP]'\r\n",
        "                        max_length = 512,           # Pad & truncate all sentences.\r\n",
        "                        pad_to_max_length = True,\r\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\r\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\r\n",
        "                   )\r\n",
        "    \r\n",
        "    # Add the encoded sentence to the list.    \r\n",
        "    input_ids.append(encoded_dict['input_ids'])\r\n",
        "    \r\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\r\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\r\n",
        "\r\n",
        "# Convert the lists into tensors.\r\n",
        "input_ids = torch.cat(input_ids, dim=0)\r\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\r\n",
        "\r\n",
        "# Create the DataLoader.\r\n",
        "# prediction_data = TensorDataset(input_ids, attention_masks, labels)\r\n",
        "prediction_data = TensorDataset(input_ids, attention_masks)\r\n",
        "prediction_sampler = SequentialSampler(prediction_data)\r\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGvF0sVm5A5r"
      },
      "source": [
        "# Prediction on test set\r\n",
        "\r\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\r\n",
        "\r\n",
        "# Put model in evaluation mode\r\n",
        "model.eval()\r\n",
        "\r\n",
        "# Tracking variables \r\n",
        "predictions , true_labels = [], []\r\n",
        "\r\n",
        "# Predict \r\n",
        "for batch in prediction_dataloader:\r\n",
        "  # Add batch to GPU\r\n",
        "  batch = tuple(t.to(device) for t in batch)\r\n",
        "  \r\n",
        "  # Unpack the inputs from our dataloader\r\n",
        "  b_input_ids, b_input_mask = batch\r\n",
        "  \r\n",
        "  # Telling the model not to compute or store gradients, saving memory and \r\n",
        "  # speeding up prediction\r\n",
        "  with torch.no_grad():\r\n",
        "      # Forward pass, calculate logit predictions\r\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \r\n",
        "                      attention_mask=b_input_mask)\r\n",
        "\r\n",
        "  logits = outputs[0]\r\n",
        "\r\n",
        "  # Move logits and labels to CPU\r\n",
        "  logits = logits.detach().cpu().numpy()\r\n",
        "  label_ids = b_labels.to('cpu').numpy()\r\n",
        "  \r\n",
        "  # Store predictions and true labels\r\n",
        "  predictions.append(logits)\r\n",
        "  true_labels.append(label_ids)\r\n",
        "\r\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPjghzLKCAVh"
      },
      "source": [
        "len(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imcArbv0CRbw"
      },
      "source": [
        "pred = np.array([])\r\n",
        "for el in predictions:\r\n",
        "  pred = np.append(pred, el[:,1], axis=0)\r\n",
        "\r\n",
        "len(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBDKD-J4CRLq"
      },
      "source": [
        "pred_edited = np.where(pred > 0, 1, 0).tolist()\r\n",
        "len(pred_edited)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrKZCuYcGX_F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXmjv5P6CQ23"
      },
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNvClo1uCXFY"
      },
      "source": [
        "import jsonlines\r\n",
        "def save_output(data, path):\r\n",
        "    with jsonlines.open('MuSeRC/test.jsonl') as reader:\r\n",
        "        lines = list(reader)\r\n",
        "    res = []\r\n",
        "    cnt = 0\r\n",
        "\r\n",
        "    for row in lines:\r\n",
        "        res_ids = {\"idx\": row[\"idx\"], \"passage\": {\"questions\": []}}\r\n",
        "\r\n",
        "        for line in row[\"passage\"][\"questions\"]:\r\n",
        "          res_line = {\"idx\": line[\"idx\"], \"answers\": []}\r\n",
        "         \r\n",
        "          for answ in line[\"answers\"]:\r\n",
        "              res_line[\"answers\"].append({\"idx\": answ[\"idx\"], \"label\": int(data[cnt])})\r\n",
        "              cnt = cnt + 1\r\n",
        "          res_ids[\"passage\"][\"questions\"].append(res_line)\r\n",
        "\r\n",
        "        res.append(res_ids)\r\n",
        "\r\n",
        "\r\n",
        "    with open(path, mode=\"w\") as file:\r\n",
        "        for line in sorted(res, key=lambda x: int(x.get(\"idx\"))):\r\n",
        "            line[\"idx\"] = int(line[\"idx\"])\r\n",
        "            file.write(f\"{json.dumps(line, ensure_ascii=False)}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ1Hw9paCYQx"
      },
      "source": [
        "save_output(pred_edited, 'submission.jsonl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7sMLA0BH9oJ"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.download('submission.jsonl') "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}